{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bddba174-6e48-47e2-a4d6-37fb36839988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[460 232 178]\n",
      "[460.02164559 232.02470266 178.02117307]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_multi import  load_house_data, run_gradient_descent \n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "# load the dataset\n",
    "#x_train,y_train=load_house_data()\n",
    "x_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "\n",
    "def compute_prediction(x,w,b):\n",
    "    m=x.shape[0]\n",
    "    prediction=np.zeros(m)\n",
    "\n",
    "    for i in range(m):\n",
    "        prediction[i]=np.dot(w,x[i]) + b\n",
    "\n",
    "    return prediction\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "# a=compute_prediction(x_train,w_init,b_init)\n",
    "# print(f'{y_train}')\n",
    "\n",
    "def compute_cost(x,y,w,b):\n",
    "    predicted_output=compute_prediction(x,w,b)\n",
    "    m=x.shape[0]\n",
    "    cost=0\n",
    "    \n",
    "    for i in range(m):\n",
    "        cost=cost + (predicted_output[i]-y[i])**2\n",
    "\n",
    "    total_cost=cost/(2*m)\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "# c=compute_cost(x_train,y_train,w_init,b_init)\n",
    "# print(f'{c}')\n",
    "\n",
    "def compute_gradient_descent_derivatives(x,y,w,b):\n",
    "    m=x.shape[0]\n",
    "    predicted_output=compute_prediction(x,w,b)\n",
    "    #print(f'{predicted_output}')\n",
    "    dj_dw1=0\n",
    "    dj_dw2=0\n",
    "    dj_dw3=0\n",
    "    dj_dw4=0\n",
    "    dj_db=0\n",
    "\n",
    "    for i in range(m):\n",
    "        dj_dw1=dj_dw1 + (predicted_output[i]-y[i]) * x[i][0]\n",
    "        dj_dw2=dj_dw2 + (predicted_output[i]-y[i]) * x[i][1]\n",
    "        dj_dw3=dj_dw3 + (predicted_output[i]-y[i]) * x[i][2]\n",
    "        dj_dw4=dj_dw4 + (predicted_output[i]-y[i]) * x[i][3]\n",
    "        dj_db=dj_db + (predicted_output[i]-y[i])\n",
    "\n",
    "    dj_dw1=(dj_dw1)/(m)\n",
    "    dj_dw2=(dj_dw2)/(m)\n",
    "    dj_dw3=(dj_dw3)/(m)\n",
    "    dj_dw4=(dj_dw4)/(m)\n",
    "    dj_db=(dj_db)/(m)\n",
    "\n",
    "    return dj_dw1,dj_dw2,dj_dw3,dj_dw4,dj_db\n",
    "\n",
    "def computing_gradient_descent(x,y,w,b,a,iterations):\n",
    "    \n",
    "    m=x.shape[0]\n",
    "    plot_cost=np.zeros(1000000)\n",
    "    plot_iteration=np.zeros(1000000)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        dj_dw1,dj_dw2,dj_dw3,dj_dw4,dj_db=compute_gradient_descent_derivatives(x,y,w,b)\n",
    "        w[0]=w[0]-a*dj_dw1\n",
    "        w[1]=w[1]-a*dj_dw2\n",
    "        w[2]=w[2]-a*dj_dw3\n",
    "        w[3]=w[3]-a*dj_dw4\n",
    "        b=b-a*dj_db\n",
    "\n",
    "        c=compute_cost(x,y,w,b)\n",
    "        #print(f'{c}')\n",
    "        plot_cost[i]=c\n",
    "        plot_iteration[i]=i+1\n",
    "\n",
    "    return w,b\n",
    "        \n",
    "        \n",
    "    # c=compute_prediction(x_train,w,b)\n",
    "    # matplt.scatter(plot_iteration,plot_cost,marker='.',c='r',label='Actual Value')\n",
    "    # matplt.plot(plot_iteration,plot_cost,c='b',label='Our Prediction')\n",
    "    # matplt.title('Cost vs Iteration')\n",
    "    # matplt.xlabel('Iterations')\n",
    "    # matplt.ylabel('Cost')\n",
    "    # matplt.legend()\n",
    "    # matplt.show()\n",
    "    # print(f'{c}')\n",
    "        \n",
    "# Function to Normalize features so I can easily find parameters w&b to make predictions\n",
    "def normalize_features(x):\n",
    "    x_mean=np.mean(x,axis=0)\n",
    "    x_std=np.std(x,axis=0)\n",
    "    x_norm=(x-x_mean)/x_std\n",
    "\n",
    "    return x_norm\n",
    "\n",
    "def linear_model_tester():\n",
    "    w=np.array([10.,10.,10.,10.])\n",
    "    b=786\n",
    "    a=0.0001\n",
    "    alpha = 5.0e-7\n",
    "    iterations=100000\n",
    "    x_norm=normalize_features(x_train)\n",
    "    # w,b=computing_gradient_descent(x_train,y_train,w,b,alpha,iterations)\n",
    "    w,b=computing_gradient_descent(x_norm,y_train,w,b,a,iterations)\n",
    "    print(f'{y_train}')\n",
    "    pre_opt=compute_prediction(x_norm,w,b)\n",
    "    print(f'{pre_opt}')    \n",
    "\n",
    "linear_model_tester()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba5b20-e998-477b-ad5f-f20cf10011ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
